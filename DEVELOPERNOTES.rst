Developer notes
===============

Additional details for setup, troubleshooting, and other procedures.


Local Solr setup
----------------
Install Solr via `brew <https://formulae.brew.sh/formula/solr>`::

    brew install solr

Copy the Solr config files in as a configset named `ppa`::

    cp -r solr_conf /opt/homebrew/opt/solr/server/solr/configsets/ppa

Create symbolic link to configsets in the Solr home directory::

    ln -s /opt/homebrew/opt/solr/server/solr/configsets /opt/homebrew/var/lib/solr/

Create a new core with the `ppa` configset (Solr must be running)::

    curl "http://localhost:8983/solr/admin/cores?action=CREATE&name=ppa&configSet=ppa"

When the configset has changed, copy in the updated Solr config files::

    cp solr_conf/* /opt/homewbrew/var/lib/solr/configsets/ppa/

Start Solr by running the following command::

    /opt/homebrew/opt/solr/bin/solr start -f


Local PostgreSQL
----------------
Install PostgreSQL via `brew <https://formulae.brew.sh/formula/postgresql@15>`::

    brew install postgresql@15

Start PostgreSQL (or restart after an ugrade)::

    brew services start postgresql@15

Add PostgreSQL to your PATH::

    echo 'export PATH="/opt/homebrew/opt/postgresql@15/bin:$PATH"' >> ~/.zshrc


Solr setup with Docker
----------------------

Create a new docker container with the Solr 9.2 image::

    docker run --name solr92 -p 8983:8983 -t solr:9.2

Copy the solr config files in as a configset named `ppa`::

    docker cp solr_conf solr92:/opt/solr/server/solr/configsets/ppa

Change ownership  of the configset files to the `solr` user::

    docker exec --user root solr92 /bin/bash -c "chown -R solr:solr /opt/solr/server/solr/configsets/ppa"

Copy the configsets to the solr data directory::

    docker exec -d solr92 cp -r /opt/solr/server/solr/configsets /var/solr/data

Create a new core with the `ppa` configset::

    curl "http://localhost:8983/solr/admin/cores?action=CREATE&name=ppa&configSet=ppa"

When the configset has changed, copy in the updated solr config files::

    docker cp solr_conf/* solr92:/var/solr/data/configsets/ppa/

Setup
-----

Solr changes not reflected in search results? ``solrconfig.xml`` must be
updated in Solr's main directory: ``solr/server/solr/[CORE]/conf/solrconfig.xml``


Getting full-text data
----------------------

To get a full copy of all PPA data for development, rsync from the
data directory on one of the staging VMs.  The majority of the content is
stored on an NFS mount point under `/mnt/nfs/cdh/prosody/data/`:
- `ht_text_pd`: HathiTrust pairtree data for PPA volumes
- `eebo_tcp`: XML and MARC records for EEBO-TCP volumes included in PPA
- `marc`: MARC records for Gale/ECCO content, in a pairtree format used for importing Gale content

Local OCR for Gale/ECCO content is on a TigerData NFS mount point:
- `/mnt/tigerdata/cdh/prosody/ppa-ocr/Gale-by-vol`

Use rsync over ssh and copy these to a local staging area, and then configure
the paths in your local settings file:
- `HATHI_DATA`: path to the top-level HathiTrust pairtree folder
- `EEBO_DATA`: path to the eebo_tcp folder
- `GALE_LOCAL_OCR`: path to Gale-by-vol local OCR content

Indexing Gale/ECCO records requires access to the Gale API; you must configure
*GALE_API_USERNAME* in local settings.

If you are working with full data, it's recommended to load a database dump from
production or staging (e.g., as generated by the replicate cdh-ansible playbook)
and then index page contents with the `index_pages` manage command.  It may
be useful to index pages by source or by id, for testing specific behavior.

Partial text data
^^^^^^^^^^^^^^^^^

If for some reason you don't want to copy all the data, you can
configure your local settings to use the PPA staging server as the
HathiTrust rsync server and import content that way.

Use these settings:
```python
HATHITRUST_RSYNC_SERVER = "pulsys@cdh-test-prosody1"
HATHITRUST_RSYNC_PATH = "/mnt/nfs/cdh/prosody/data/ht_text_pd"
```

You should then be able to use the `hathi_add` manage command or
the admin interface to import specific HathiTrust records by id.
(The application will still use the HathiTrust bibliographic API for metadata.)

Gale/ECCO records can also be imported by id using the `gale_import`
manage command.  Page content will pull from local OCR content when available.

EEBO-TCP records can be imported using the `eebo_import` script; this
requires a CSV file with the records to be imported.


Updating HathiTrust records and generating a fresh text corpus
--------------------------------------------------------------

These commands should be run on the production server as the deploy user
with the python virtual environment activated.

Update all HathiTrust documents with rsync::

    python manage.py hathi_rsync

This file will generate a csv report of the files that were updated.
Use the resulting file to get a list of ids that need to be indexed:

    cut -f 1 -d, ppa_rsync_changes_[TIMESTAMP].csv | sort | uniq | tail -n +2 > htids.txt

Index pages for the documents that were updated via rsync to make sure
Solr has all the updated page content::

    python manage.py index_pages `cat htids.txt`

Generate a new text corpus::

    python manage.py generate_textcorpus

Use rsync to copy the generated corpus output to a local machine and
optionally also upload to TigerData.

If you need to filter the corpus to a smaller set of records, use the
filter utility script in the ppa-nlp repo / corppa python library
(currently in development branch.)


Indexing with multiprocessing
-----------------------------

To run the multiprocessing page index script (`index_pages`) on MacOS versions past High Sierra, you must disable a security feature that restricts multithreading.
Set this environment variable to override it: `OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES`

For more details, see `stack overflow <https://stackoverflow.com/questions/50168647/multiprocessing-causes-python-to-crash-and-gives-an-error-may-have-been-in-progr/52230415#52230415>`_.


Postgresql setup
---------------

To create a new postgres database and user for development::

    psql -d postgres -c "DROP DATABASE ppa;"
    psql -d postgres -c "DROP ROLE ppa;"
    psql -d postgres -c "CREATE ROLE ppa WITH CREATEDB LOGIN PASSWORD 'ppa';"
    psql -d postgres -U ppa -c "CREATE DATABASE ppa;"

To replace a local development database with a dump of production data::

    psql -d postgres -c "DROP DATABASE cdh_ppa;"
    psql -d postgres -c "CREATE DATABASE cdh_ppa;"
    psql cdh_ppa < data/13_daily_cdh_ppa_cdh_ppa_2023-01-11.Wednesday.sql


Updating Wagtail test fixture
-----------------------------

We use a fixture in `ppa/common/fixtures/wagtail_pages.json` for some wagtail unit tests.
To update this to reflect changes in new versions of wagtail:

1. Create an empty database to use for migrated the fixture.
2. Check out a version of the codebase before any new migrations have been applied,
and run migrations up to that point on the new database (`python manage.py migrate`)
3. Remove preloaded wagtail content from the database using python console or web interface.
4. Check out the new version of the code with the updated version of wagtail.
5. Run migrations.
6. Exported the migrated fixture data back to the fixture file. It's essential
to use the `--natural-foreign` option::

    ./manage.py dumpdata --natural-foreign wagtailcore.site wagtailcore.page wagtailcore.revision pages editorial auth.User --indent 4 > ppa/common/fixtures/wagtail_pages.json

7. Remove any extra user accounts from the fixture (like `script`)
8. Use `git diff` to check for any other major changes.


Testing local DocRaptor PDF generation
--------------------------------------

In order for DocRaptor to read any content, you must open your localhost to the
public with a service like Cloudflare Tunnel, e.g.::

    npx cloudflared tunnel --url http://localhost:8000

Then in Wagtail Site settings, set the default Site's hostname to the tunnel's
public hostname (no protocol/slashes), and port 80. That way,
``GeneratePdfPanel.BoundPanel.instance.full_url`` resolves to a public URL.

Finally, set your ALLOWED_HOSTS setting to allow traffic via that domain,
or simply set ``ALLOWED_HOSTS = ["*"]``.

Note that this will not work in Webpack dev mode.

When finished, set the default Site back to ``localhost`` and port 8000.
