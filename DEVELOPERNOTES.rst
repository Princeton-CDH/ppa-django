Developer notes
===============


Local Solr setup
----------------
Install Solr via `brew <https://formulae.brew.sh/formula/solr>`::

    brew install solr

Copy the Solr config files in as a configset named `ppa`::

    cp -r solr_conf /opt/homebrew/opt/solr/server/solr/configsets/ppa

Create symbolic link to configsets in the Solr home directory::

    ln -s /opt/homebrew/opt/solr/server/solr/configsets /opt/homebrew/var/lib/solr/

Create a new core with the `ppa` configset (Solr must be running)::

    curl "http://localhost:8983/solr/admin/cores?action=CREATE&name=ppa&configSet=ppa"

When the configset has changed, copy in the updated Solr config files::

    cp solr_conf/* /opt/homewbrew/var/lib/solr/configsets/ppa/

Start Solr by running the following command::

    /opt/homebrew/opt/solr/bin/solr start -f


Local PostgreSQL
----------------
Install PostgreSQL via `brew <https://formulae.brew.sh/formula/postgresql@15>`::

    brew install postgresql@15

Start PostgreSQL (or restart after an ugrade)::

    brew services start postgresql@15

Add PostgreSQL to your PATH::

    echo 'export PATH="/opt/homebrew/opt/postgresql@15/bin:$PATH"' >> ~/.zshrc


Solr setup with Docker
----------------------

Create a new docker container with the Solr 9.2 image::

    docker run --name solr92 -p 8983:8983 -t solr:9.2

Copy the solr config files in as a configset named `ppa`::

    docker cp solr_conf solr92:/opt/solr/server/solr/configsets/ppa

Change ownership  of the configset files to the `solr` user::

    docker exec --user root solr92 /bin/bash -c "chown -R solr:solr /opt/solr/server/solr/configsets/ppa"

Copy the configsets to the solr data directory::

    docker exec -d solr92 cp -r /opt/solr/server/solr/configsets /var/solr/data

Create a new core with the `ppa` configset::

    curl "http://localhost:8983/solr/admin/cores?action=CREATE&name=ppa&configSet=ppa"

When the configset has changed, copy in the updated solr config files::

    docker cp solr_conf/* solr92:/var/solr/data/configsets/ppa/

Setup
-----

Solr changes not reflected in search results? ``solrconfig.xml`` must be
updated in Solr's main directory: ``solr/server/solr/[CORE]/conf/solrconfig.xml``


Getting full-text data
----------------------

Choose Your Import Strategy
^^^^^^^^^^^^^^^^^^^^^^^^^^^

For Quick Testing:

- Import volumes individually by ID imports, e.g. ``python manage.py hathi_add [id]``
- Start with 1-2 items to verify configuration

For Development Work (Import Partial data):

- Import 100-500 representative items using file imports
- Mix sources to test different functionality

For Full Development Environment (Import full data):

-  Follow the full data workflow
-  Import complete datasets for realistic testing

Configuration Priority:

- Configure required paths (``HATHI_DATA``, ``EEBO_DATA``, ``MARC_DATA``)
- Configure API access (``GALE_API_USERNAME``)
- Test with individual imports first
- Scale up to file/CSV imports

All full-text data
^^^^^^^^^^^^^^^^^^

To get a full copy of all PPA data for development, use ``rsync`` to copy
files from the data directory on one of the staging VMs.  The majority of 
the content is stored on an NFS mount point under ``/mnt/nfs/cdh/prosody/data/``:

- ``ht_text_pd``: HathiTrust pairtree data for PPA volumes
- ``eebo_tcp``: XML and MARC records for EEBO-TCP volumes included in PPA
- ``marc``: MARC records for Gale/ECCO content, in a pairtree format used for importing Gale content

Local OCR for Gale/ECCO content is on a TigerData NFS mount point:

- ``/mnt/tigerdata/cdh/prosody/ppa-ocr/Gale-by-vol``

Use rsync over ssh and copy these to a local staging area, and then configure
the paths in your local settings file:

- ``HATHI_DATA``: path to the top-level HathiTrust pairtree folder
- ``EEBO_DATA``: path to the eebo_tcp folder
- ``MARC_DATA``: path to MARC pairtree data for Gale records (required for import)
- ``GALE_LOCAL_OCR``: path to Gale-by-vol local OCR content (optional)

Indexing Gale/ECCO records requires access to the Gale API; you must configure
*GALE_API_USERNAME* in local settings.

If you are working with full data, it's recommended to load a database dump from
production or staging (e.g., as generated by the cdh-ansible replicate playbook),
index work-level data to Solr with the ``index`` manage command::

    python manage.py index -i work

Then index page contents with the ``index_pages`` manage command.  It may
be useful to index pages from a specific source or by id, to test specific behavior
or load a targeted subset of page content::

    python manage.py index_pages

Partial text data
^^^^^^^^^^^^^^^^^

PPA draws content from three different sources. Depending on the development
work you are doing, you may not need all three.

HathiTrust
""""""""""

The HathiTrust data is fairly large; for many development tasks, it is
sufficient to work with a subset of the data. To import specific volumes
available in PPA staging, you can configure your local settings to use the
PPA staging server as the HathiTrust rsync server:

Use these settings::

    HATHITRUST_RSYNC_SERVER = "pulsys@cdh-test-prosody1.princeton.edu"
    HATHITRUST_RSYNC_PATH = "/mnt/nfs/cdh/prosody/data/ht_text_pd"

You should then be able to use the ``hathi_add`` manage command or
the admin interface to import specific HathiTrust records by id.
Note that the application will make calls to the HathiTrust bibliographic API
for metadata, which is used in tandem with local full-text content.

Example (specify one or more HathiTrust IDs)::

    python manage.py hathi_add njp.32101068970508

Gale/ECCO
""""""""""

Gale/ECCO records can also be imported by id using the ``gale_import``
manage command. Import requires the ``MARC_DATA`` path and MARC pairtree data.
Page content will be pulled from local OCR content when
the ``GALE_LOCAL_OCR`` path is configured and files are available.

Example (specify one or more Gale IDs)::

    python manage.py gale_import CW0116618490

Access to the Gale API requires a ``GALE_API_USERNAME`` to be configured.
This configuration can be found in the local settings file on the staging
and production servers, and is also available as an encrypted variable in
`cdh-ansible <https://github.com/Princeton-CDH/cdh-ansible/>`_. With
a working local install of cdh-ansible with the ansible vault password, run::

    ./bin/vault_vars.py decrypt inventory/group_vars/prosody/vault.yml


EEBO-TCP
""""""""

EEBO-TCP records can be imported using the ``eebo_import`` script; this
requires the ``EEBO_DATA`` folder and configuration and a CSV file with
the records to be imported.  A copy of the CSV used for the production
import is available in this repository
at ``scripts/eebo_works.csv``.

Import command::

    python manage.py eebo_import scripts/eebo_works.csv


Verification steps
^^^^^^^^^^^^^^^^^

The easiest way to check the data imported into your local database is 
by using the Django Admin web interface. 

- Start the development server: ``python manage.py runserver``
- Create user account if needed: ``python manage.py createcasuser --admin netid``
- Go to http://localhost:8000/admin/
- Navigate to "Archive" â†’ "Digitized works"
- Filter by source to see imported items
- Check individual records for page counts and metadata

To check contents indexed in Solr:

-  Go to http://localhost:8983/solr/
- Select 'ppa' in 'Core Selector'
- The 'Statistics' section shows the number of indexed items
- Use the default query (``*:*``) to see all content, or filter to see
  specific subsets of content (e.g. ``item_type:page`` or ``item_type:work``)


Updating HathiTrust records and generating a fresh text corpus
--------------------------------------------------------------

These commands should be run on the production server as the deploy user
with the python virtual environment activated.

Update all HathiTrust documents with rsync::

    python manage.py hathi_rsync

This file will generate a csv report of the files that were updated.
Use the resulting file to get a list of ids that need to be indexed::

    cut -f 1 -d, ppa_rsync_changes_[TIMESTAMP].csv | sort | uniq | tail -n +2 > htids.txt

Index pages for the documents that were updated via rsync to make sure
Solr has all the updated page content::

    python manage.py index_pages `cat htids.txt`

Generate a new text corpus::

    python manage.py generate_textcorpus

Use rsync to copy the generated corpus output to a local machine and
optionally also upload to TigerData.

If you need to filter the corpus to a smaller set of records, use the
`filter utility script <https://princeton-cdh.github.io/corppa/eop-docs.html#filter-utility>`_
in the `corppa python library <https://github.com/Princeton-CDH/corppa>`_.


Indexing with multiprocessing
-----------------------------

To run the multiprocessing page index script (`index_pages`) on MacOS versions past High Sierra, you must disable a security feature that restricts multithreading.
Set this environment variable to override it: `OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES`

For more details, see `stack overflow <https://stackoverflow.com/questions/50168647/multiprocessing-causes-python-to-crash-and-gives-an-error-may-have-been-in-progr/52230415#52230415>`_.


Postgresql setup
---------------

To create a new postgres database and user for development::

    psql -d postgres -c "DROP DATABASE ppa;"
    psql -d postgres -c "DROP ROLE ppa;"
    psql -d postgres -c "CREATE ROLE ppa WITH CREATEDB LOGIN PASSWORD 'ppa';"
    psql -d postgres -U ppa -c "CREATE DATABASE ppa;"

To replace a local development database with a dump of production data::

    psql -d postgres -c "DROP DATABASE cdh_ppa;"
    psql -d postgres -c "CREATE DATABASE cdh_ppa;"
    psql cdh_ppa < data/13_daily_cdh_ppa_cdh_ppa_2023-01-11.Wednesday.sql


Updating Wagtail test fixture
-----------------------------

We use a fixture in `ppa/common/fixtures/wagtail_pages.json` for some wagtail unit tests.
To update this to reflect changes in new versions of wagtail:

1. Create an empty database to use for migrated the fixture.
2. Check out a version of the codebase before any new migrations have been applied,
and run migrations up to that point on the new database (`python manage.py migrate`)
3. Remove preloaded wagtail content from the database using python console or web interface.
4. Check out the new version of the code with the updated version of wagtail.
5. Run migrations.
6. Exported the migrated fixture data back to the fixture file. It's essential
to use the `--natural-foreign` option::

    ./manage.py dumpdata --natural-foreign wagtailcore.site wagtailcore.page wagtailcore.revision pages editorial auth.User --indent 4 > ppa/common/fixtures/wagtail_pages.json

7. Remove any extra user accounts from the fixture (like `script`)
8. Use `git diff` to check for any other major changes.


Testing local DocRaptor PDF generation
--------------------------------------

In order for DocRaptor to read any content, you must open your localhost to the
public with a service like Cloudflare Tunnel, e.g.::

    npx cloudflared tunnel --url http://localhost:8000

Then in Wagtail Site settings, set the default Site's hostname to the tunnel's
public hostname (no protocol/slashes), and port 80. That way,
``GeneratePdfPanel.BoundPanel.instance.full_url`` resolves to a public URL.

Finally, set your ALLOWED_HOSTS setting to allow traffic via that domain,
or simply set ``ALLOWED_HOSTS = ["*"]``.

Note that this will not work in Webpack dev mode.

When finished, set the default Site back to ``localhost`` and port 8000.
